# GPT2025

[![License: MIT](https://img.shields.io/badge/License-MIT-green.svg)](LICENSE)
[![Stars](https://img.shields.io/github/stars/dustinwloring1988/gpt2025?style=social)](https://github.com/dustinwloring1988/gpt2025/stargazers)
[![Python](https://img.shields.io/badge/python-3.8%2B-blue.svg)](https://www.python.org/downloads/)

**GPT2025** is an advanced, research-grade implementation of a GPT-style language model that integrates cutting-edge efficiency techniques including FlashAttention, RMSNorm, RoPE/NoPE hybrid positional embeddings, and SwiGLU activations. This repository also includes support scripts for large-scale pretraining and evaluation on datasets like FineWeb and HellaSwag.

---

## ğŸš€ Features

- ğŸ” **Hybrid RoPE + NoPE Positional Embeddings**
- âš¡ **FlashAttention 3** integration (optional)
- ğŸ§  **RMSNorm** over traditional LayerNorm
- ğŸ§¬ **SwiGLU Activation** support
- ğŸ§® **KV Caching** for efficient generation
- ğŸ§° **From-pretrained GPT-2 compatibility**
- ğŸ‹ï¸â€â™‚ï¸ **Efficient MLP and attention layers**
- ğŸ”„ **Gradient checkpointing (optional)**
- ğŸ“‰ **Cosine annealing learning rate scheduler**

---

## ğŸ“ Repository Structure

```text
.
â”œâ”€â”€ enhanced_gpt2_model.py   # Core GPT2025 model with all optimizations
â”œâ”€â”€ fineweb.py               # Dataset preprocessor for FineWeb-Edu
â”œâ”€â”€ hellaswag.py             # Script for downloading and evaluating HellaSwag
â””â”€â”€ README.md                # This file
````

---

## ğŸ§  Model Variants

The `create_optimized_model` function lets you choose between different model sizes and positional encoding strategies:

```python
model = create_optimized_model(model_size='small', position_strategy='hybrid')
```

Available options:

* `model_size`: `'small'`, `'medium'`, `'large'`
* `position_strategy`: `'hybrid'`, `'rope'`, `'nope'`

---

## ğŸ“¦ Pretraining: FineWeb-Edu

The `fineweb.py` script tokenizes and shards the [FineWeb-Edu](https://huggingface.co/datasets/HuggingFaceFW/fineweb-edu) dataset:

```bash
python fineweb.py
```

This will download and preprocess the dataset into the `edu_fineweb10B` directory as NumPy `.npy` shards.

---

## ğŸ“Š Evaluation: HellaSwag

To evaluate GPT2-family models on HellaSwag:

```bash
python hellaswag.py --model_type gpt2-xl --device cuda
```

Supports both `acc` and `acc_norm` scoring strategies for multiple-choice evaluation.

---

## ğŸ› ï¸ Requirements

* Python 3.8+
* PyTorch 2.x
* `transformers`, `datasets`, `tiktoken`, `tqdm`, `numpy`
* Optional: FlashAttention, Triton, A100 for optimal performance

Install with pip:

```bash
pip install torch transformers datasets tiktoken tqdm numpy
```

---

## ğŸ§ª Example Forward Pass

```python
model = create_optimized_model('small', 'hybrid').to('cuda')
input_ids = torch.randint(0, model.config.vocab_size, (4, 512)).to('cuda')
logits, loss = model(input_ids)
```

---

## ğŸ“ˆ Training Utilities

The model provides:

* `configure_optimizers`: optimizer setup with parameter group separation
* `reset_cache`: clears KV-cache for generation
* `estimate_mfu`: estimate model FLOP utilization (A100 reference)

---

## ğŸ“ License

MIT License

---

## ğŸ™ Acknowledgments

Built with insights from NanoGPT, FlashAttention, HellaSwag, and the open-source transformer community.
